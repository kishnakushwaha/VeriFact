"""
Verdict Engine Module with Accuracy Improvements

Computes the final verdict from collected evidence using weighted scoring.
Aggregates support/refute evidence and outputs confidence scores.
Includes explanation generation for transparency.

Accuracy improvements:
- Tuned decision thresholds (+0.35/-0.35 instead of Â±0.4)
- Enhanced weighting formula with similarity boost
"""

import math
import logging
from typing import Dict, List, Tuple

logger = logging.getLogger(__name__)

# Tuned thresholds for better precision/recall balance
THRESHOLD_TRUE = 0.35   # Was 0.4 - lowered for better recall
THRESHOLD_FALSE = -0.35  # Was -0.4 - raised for better recall


def sigmoid(x: float) -> float:
    """
    Sigmoid function to normalize scores to 0-1 range.
    
    Args:
        x: Input value
        
    Returns:
        Value between 0 and 1
    """
    return 1 / (1 + math.exp(-x))


def compute_weighted_score(evidence: Dict) -> Tuple[float, str]:
    """
    Calculate weighted score for a single piece of evidence.
    
    Enhanced formula: similarity Ã— stance_score Ã— stance_weight Ã— source_weight Ã— similarity_boost
    
    Args:
        evidence: Dict with 'similarity', 'stance', 'stance_score', 'source_weight'
        
    Returns:
        Tuple of (weighted_score, stance_direction)
    """
    similarity = evidence["similarity"]
    stance_score = evidence["stance_score"]
    stance = evidence["stance"]

    # Stance weight: +1 for support, -1 for refute, 0 for neutral
    if stance == "supports":
        stance_w = +1
    elif stance == "refutes":
        stance_w = -1
    else:
        stance_w = 0  # discusses / neutral
    
    source_weight = evidence.get("source_weight", 1.0)
    
    # Accuracy improvement: Boost high-similarity evidence more
    similarity_boost = 1.0 + (similarity - 0.5) * 0.5  # 0.75x to 1.25x based on similarity
    
    score = similarity * stance_score * stance_w * source_weight * similarity_boost

    return score, stance


def build_explanation(evidences: List[Dict], scores: List[Tuple[float, str]], 
                      net_score: float, verdict: str) -> Dict:
    """
    Build a structured explanation of the verdict reasoning.
    
    Args:
        evidences: List of evidence dictionaries
        scores: List of (score, stance) tuples
        net_score: Final aggregated score
        verdict: The verdict string
        
    Returns:
        Dict with 'steps', 'breakdown', and 'decision_reason'
    """
    # Count stances
    support_count = sum(1 for _, s in scores if s == "supports")
    refute_count = sum(1 for _, s in scores if s == "refutes")
    neutral_count = sum(1 for _, s in scores if s not in ("supports", "refutes"))
    
    # Calculate weights by stance
    support_weight = round(sum(sc for sc, st in scores if st == "supports"), 2)
    refute_weight = round(sum(sc for sc, st in scores if st == "refutes"), 2)
    
    # Count trusted sources
    trusted_count = sum(1 for e in evidences if e.get("source_weight", 1.0) > 1.0)
    
    # Count multi-sentence evidence
    multi_sent_count = sum(1 for e in evidences if e.get("supporting_sentences"))
    
    # Build reasoning steps
    steps = [
        {
            "step": 1,
            "title": "Evidence Collection",
            "detail": f"Found {len(evidences)} relevant source{'s' if len(evidences) != 1 else ''}",
            "icon": "ðŸ”"
        },
        {
            "step": 2,
            "title": "Stance Analysis",
            "detail": f"{support_count} support, {refute_count} refute, {neutral_count} neutral",
            "icon": "âš–ï¸"
        },
        {
            "step": 3,
            "title": "Credibility Weighting",
            "detail": f"{trusted_count} trusted source{'s' if trusted_count != 1 else ''} (Reuters, BBC, etc.)",
            "icon": "ðŸ†"
        },
        {
            "step": 4,
            "title": "Score Calculation",
            "detail": f"Net score: {net_score:+.2f} (support: {support_weight:+.2f}, refute: {refute_weight:+.2f})",
            "icon": "ðŸ“Š"
        }
    ]
    
    # Decision reason based on verdict
    if verdict == "LIKELY TRUE":
        decision_reason = f"Score ({net_score:+.2f}) exceeds +{THRESHOLD_TRUE} threshold. The majority of credible evidence supports this claim."
    elif verdict == "LIKELY FALSE":
        decision_reason = f"Score ({net_score:+.2f}) is below {THRESHOLD_FALSE} threshold. The majority of credible evidence contradicts this claim."
    elif verdict == "UNVERIFIED":
        decision_reason = "No relevant evidence was found to verify or refute this claim."
    else:  # MIXED / MISLEADING
        decision_reason = f"Score ({net_score:+.2f}) is between {THRESHOLD_FALSE} and +{THRESHOLD_TRUE}. Evidence is conflicting or inconclusive."
    
    # Add final verdict step
    steps.append({
        "step": 5,
        "title": "Verdict",
        "detail": decision_reason,
        "icon": "âœ…" if verdict == "LIKELY TRUE" else "âŒ" if verdict == "LIKELY FALSE" else "âš ï¸"
    })
    
    return {
        "steps": steps,
        "breakdown": {
            "support_count": support_count,
            "refute_count": refute_count,
            "neutral_count": neutral_count,
            "support_weight": support_weight,
            "refute_weight": refute_weight,
            "trusted_sources": trusted_count,
            "total_sources": len(evidences),
            "multi_sentence_evidence": multi_sent_count
        },
        "decision_reason": decision_reason,
        "threshold_info": f"Thresholds: TRUE > +{THRESHOLD_TRUE}, FALSE < {THRESHOLD_FALSE}, MIXED in between"
    }


def compute_final_verdict(evidences: List[Dict], include_explanation: bool = True) -> Dict:
    """
    Compute the final verdict based on weighted aggregation of all evidence.
    
    Decision Thresholds (tuned for accuracy):
        - net_score > 0.35  â†’ LIKELY TRUE
        - net_score < -0.35 â†’ LIKELY FALSE
        - otherwise         â†’ MIXED / MISLEADING
        - no evidence       â†’ UNVERIFIED
    
    Args:
        evidences: List of evidence dictionaries
        include_explanation: Whether to include detailed explanation
        
    Returns:
        Dict with 'verdict', 'confidence', 'net_score', and optionally 'explanation'
    """
    if not evidences:
        logger.info("No evidence found - returning UNVERIFIED")
        result = {
            "verdict": "UNVERIFIED",
            "confidence": 0.0,
            "net_score": 0
        }
        if include_explanation:
            result["explanation"] = build_explanation([], [], 0, "UNVERIFIED")
        return result

    # Compute scores with stance info
    score_data = [compute_weighted_score(e) for e in evidences]
    scores = [s[0] for s in score_data]
    net_score = sum(scores)

    confidence = sigmoid(abs(net_score))

    # Decision thresholds (tuned for better accuracy)
    if net_score > THRESHOLD_TRUE:
        verdict = "LIKELY TRUE"
    elif net_score < THRESHOLD_FALSE:
        verdict = "LIKELY FALSE"
    else:
        verdict = "MIXED / MISLEADING"

    logger.info(f"Verdict: {verdict} (score: {net_score:.3f}, confidence: {confidence:.3f})")

    result = {
        "verdict": verdict,
        "confidence": round(confidence, 3),
        "net_score": round(net_score, 3)
    }
    
    if include_explanation:
        result["explanation"] = build_explanation(evidences, score_data, net_score, verdict)
    
    return result
